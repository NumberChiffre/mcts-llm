{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "import dspy\n",
    "from dspy.datasets.gsm8k import GSM8K, gsm8k_metric\n",
    "from dspy.evaluate import Evaluate\n",
    "from dspy.teleprompt import MIPROv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "from mcts_llm.mctsr import MCTSr\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "logging.basicConfig(level=logging.WARNING)\n",
    "logging.getLogger(\"mcts-llm\").setLevel(logging.INFO)\n",
    "\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"The user will provide a problem. Solve the problem. Think step by step.\"\n",
    "ollama = dspy.OllamaLocal(\n",
    "    model=\"qwen2.5:7b-instruct\", \n",
    "    model_type=\"chat\",\n",
    "    temperature=1.0,\n",
    "    max_tokens=1024,\n",
    "    num_ctx=1024,\n",
    "    system=system_prompt,\n",
    "    timeout_s=600\n",
    ")\n",
    "openai = dspy.OpenAI(\n",
    "    model=\"deepseek-chat\", \n",
    "    model_type=\"chat\",\n",
    "    api_key=os.environ[\"DEEPSEEK_API_KEY\"], \n",
    "    base_url=os.environ[\"DEEPSEEK_BASE_URL\"], \n",
    "    temperature=1.0,\n",
    "    max_tokens=4096\n",
    ")\n",
    "dspy.settings.configure(lm=ollama, experimental=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsm8k = GSM8K()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsm8k_trainset = [\n",
    "    dspy.Example(\n",
    "        problem=example['question'], \n",
    "        gold_reasoning=example['gold_reasoning'],\n",
    "        answer=example['answer']\n",
    "    ).with_inputs(\"problem\") for example in gsm8k.train\n",
    "]\n",
    "np.random.shuffle(gsm8k_trainset)\n",
    "gsm8k_trainset[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gsm8k_testset = [\n",
    "    dspy.Example(\n",
    "        problem=example['question'], \n",
    "        gold_reasoning=example['gold_reasoning'],\n",
    "        answer=example['answer']\n",
    "    ).with_inputs(\"problem\") for example in gsm8k.test\n",
    "]\n",
    "np.random.shuffle(gsm8k_testset)\n",
    "gsm8k_testset[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate = Evaluate(\n",
    "    devset=gsm8k_testset[:20], \n",
    "    metric=gsm8k_metric, \n",
    "    num_threads=os.cpu_count(), \n",
    "    display_progress=True,\n",
    "    display_table=20,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = MIPROv2(\n",
    "    prompt_model=openai,\n",
    "    task_model=ollama,\n",
    "    metric=gsm8k_metric,\n",
    "    init_temperature=0.5,\n",
    "    num_candidates=7,\n",
    "    num_threads=os.cpu_count(),\n",
    "    verbose=True\n",
    ")\n",
    "miprov2_mctsr = optimizer.compile(\n",
    "    MCTSr(), \n",
    "    trainset=gsm8k_trainset[:50],\n",
    "    requires_permission_to_run=False,\n",
    "    num_trials=15,\n",
    "    max_labeled_demos=0, \n",
    "    max_bootstrapped_demos=0\n",
    ")\n",
    "miprov2_mctsr.save(\"miprov2_mctsr.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluate(miprov2_mctsr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_logs = miprov2_mctsr.trial_logs\n",
    "trial_numbers = list(trial_logs.keys())\n",
    "scores = [trial_logs[trial]['score'] for trial in trial_numbers]\n",
    "pruning_status = [trial_logs[trial]['pruned'] for trial in trial_numbers]\n",
    "\n",
    "plt.figure(figsize=(5, 3))\n",
    "for trial_number, score, pruned in zip(trial_numbers, scores, pruning_status):\n",
    "    if pruned:\n",
    "        plt.scatter(trial_number, score, color='grey', label='Pruned Batch' if 'Pruned Batch' not in plt.gca().get_legend_handles_labels()[1] else \"\")\n",
    "    else:\n",
    "        plt.scatter(trial_number, score, color='green', label='Successful Batch' if 'Successful Batch' not in plt.gca().get_legend_handles_labels()[1] else \"\")\n",
    "\n",
    "plt.xlabel('Batch Number')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Batch Scores')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_score = 0\n",
    "\n",
    "def get_signature(predictor):\n",
    "    if (hasattr(predictor, 'extended_signature')):\n",
    "        return predictor.extended_signature\n",
    "    elif (hasattr(predictor, 'signature')):\n",
    "        return predictor.signature\n",
    "\n",
    "print(f\"Baseline program | Score: {best_score}:\")\n",
    "for i,predictor in enumerate(MCTSr().predictors()):\n",
    "    print(f\"Prompt {i+1} Instruction: {get_signature(predictor).instructions}\")\n",
    "print()\n",
    "\n",
    "print(\"----------------\")\n",
    "\n",
    "for trial_num in miprov2_mctsr.trial_logs:\n",
    "    program_score = miprov2_mctsr.trial_logs[trial_num][\"score\"]\n",
    "    program_pruned = miprov2_mctsr.trial_logs[trial_num][\"pruned\"]\n",
    "    best_score = program_score\n",
    "    best_program_so_far = miprov2_mctsr.trial_logs[trial_num][\"program\"]\n",
    "    print(f\"Best program after {trial_num} batches | Score: {best_score}:\")\n",
    "    for i,predictor in enumerate(best_program_so_far.predictors()):\n",
    "        print(f\"Prompt {i+1} Instruction: {get_signature(predictor).instructions}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mcts-llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
